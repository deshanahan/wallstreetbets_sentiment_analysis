{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371a4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\desha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Link</th>\n",
       "      <th>Time</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Enjoy you salty bastard :)</td>\n",
       "      <td>1599700428</td>\n",
       "      <td>/r/wallstreetbets/comments/ipnztr/what_are_you...</td>\n",
       "      <td>2020-09-09 21:13:48</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"A red sun rises, blood has been spilled this ...</td>\n",
       "      <td>1599700430</td>\n",
       "      <td>/r/wallstreetbets/comments/ipniz0/hey_if_its_f...</td>\n",
       "      <td>2020-09-09 21:13:50</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this information was not useful to me</td>\n",
       "      <td>1599700430</td>\n",
       "      <td>/r/wallstreetbets/comments/ipnztr/what_are_you...</td>\n",
       "      <td>2020-09-09 21:13:50</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.3412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3am ‚Äúthey‚Äù are going to buy the shit out of an...</td>\n",
       "      <td>1599700436</td>\n",
       "      <td>/r/wallstreetbets/comments/ipnztr/what_are_you...</td>\n",
       "      <td>2020-09-09 21:13:56</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.6124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Nah, you didn‚Äôt know. Now you know üòÅ</td>\n",
       "      <td>1599700437</td>\n",
       "      <td>/r/wallstreetbets/comments/ipnztr/what_are_you...</td>\n",
       "      <td>2020-09-09 21:13:57</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.1027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Comment   Timestamp  \\\n",
       "0           0                         Enjoy you salty bastard :)  1599700428   \n",
       "1           1  \"A red sun rises, blood has been spilled this ...  1599700430   \n",
       "2           2              this information was not useful to me  1599700430   \n",
       "3           3  3am ‚Äúthey‚Äù are going to buy the shit out of an...  1599700436   \n",
       "4           4               Nah, you didn‚Äôt know. Now you know üòÅ  1599700437   \n",
       "\n",
       "                                                Link                 Time  \\\n",
       "0  /r/wallstreetbets/comments/ipnztr/what_are_you...  2020-09-09 21:13:48   \n",
       "1  /r/wallstreetbets/comments/ipniz0/hey_if_its_f...  2020-09-09 21:13:50   \n",
       "2  /r/wallstreetbets/comments/ipnztr/what_are_you...  2020-09-09 21:13:50   \n",
       "3  /r/wallstreetbets/comments/ipnztr/what_are_you...  2020-09-09 21:13:56   \n",
       "4  /r/wallstreetbets/comments/ipnztr/what_are_you...  2020-09-09 21:13:57   \n",
       "\n",
       "   Year  Month  Day  Hour  Sentiment  \n",
       "0  2020      9    9    21     0.4019  \n",
       "1  2020      9    9    21    -0.1761  \n",
       "2  2020      9    9    21    -0.3412  \n",
       "3  2020      9    9    21     0.6124  \n",
       "4  2020      9    9    21    -0.1027  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from twitter_auth import consumer_key, consumer_secret, access_token, access_token_secret\n",
    "from tweepy.streaming import StreamListener\n",
    "import yfinance\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "historic_sentiment = pd.read_csv('wallstreetbets_big_df.csv')\n",
    "historic_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "22d1cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(user_stock):\n",
    "    user_df = historic_sentiment.loc[historic_sentiment['Comment'].str.lower().str.contains(user_stock.lower(), na=False)]\n",
    "    user_df.reset_index(inplace=True, drop=True)\n",
    "    user_time_period = ['Year', 'Month', 'Day']\n",
    "    user_df = user_df.groupby(user_time_period).mean('Sentiment')\n",
    "    sentiment_df = pd.Series(user_df['Sentiment'])\n",
    "    sentiment_df = sentiment_df.reset_index()\n",
    "    sentiment_df['Date'] = [f'''{sentiment_df['Year'][i]}-{sentiment_df['Month'][i]}-{sentiment_df['Day'][i]}''' for i in range(len(sentiment_df))]\n",
    "    sentiment_df['Date'] = pd.to_datetime(sentiment_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    sentiment_df = sentiment_df.set_index('Date')\n",
    "    \n",
    "\n",
    "    start_time = f'''{user_df['Sentiment'].index[0][0]}-{user_df['Sentiment'].index[0][1]}-{user_df['Sentiment'].index[0][2]}'''\n",
    "    end_time = f'''{user_df['Sentiment'].index[-1][0]}-{user_df['Sentiment'].index[-1][1]}-{user_df['Sentiment'].index[-1][2]}'''\n",
    "\n",
    "    y_ticker = yfinance.Ticker(user_stock)\n",
    "    close = y_ticker.history(start=start_time, end=end_time, interval='1d')['Close']\n",
    "\n",
    "    close_df = pd.DataFrame(close, columns=['Close'])\n",
    "    close_df = close_df.reset_index()\n",
    "    close_df['Year'] = close_df['Date'].dt.strftime('%Y')\n",
    "    close_df['Month'] = close_df['Date'].dt.strftime('%m')\n",
    "    close_df['Day'] = close_df['Date'].dt.strftime('%d')\n",
    "    close_df_user_time_period = close_df.groupby(user_time_period).mean()\n",
    "    close_df_user_time_period = close_df_user_time_period.reset_index()\n",
    "    close_df_user_time_period['Date'] = [f'''{close_df_user_time_period['Year'][i]}-{close_df_user_time_period['Month'][i]}-{close_df_user_time_period['Day'][i]}''' for i in range(len(close_df_user_time_period))]\n",
    "    close_df_user_time_period = close_df_user_time_period.set_index('Date')\n",
    "    close_df_user_time_period['Pct Change'] = close_df_user_time_period['Close'].pct_change()\n",
    "    close_df_user_time_period = close_df_user_time_period.dropna()\n",
    "    close_df_user_time_period['Pct Change Very High Class'] = np.where(close_df_user_time_period['Pct Change'] > close_df_user_time_period['Pct Change'].describe()['75%'], 'Very High', 0)\n",
    "    close_df_user_time_period['Pct Change High Class'] = np.where(close_df_user_time_period['Pct Change'].between(close_df_user_time_period['Pct Change'].describe()['50%'], close_df_user_time_period['Pct Change'].describe()['75%']), 'High', 0)\n",
    "    close_df_user_time_period['Pct Change Low Class'] = np.where(close_df_user_time_period['Pct Change'].between(close_df_user_time_period['Pct Change'].describe()['25%'], close_df_user_time_period['Pct Change'].describe()['50%']), 'Low', 0)\n",
    "    close_df_user_time_period['Pct Change Very Low Class'] = np.where(close_df_user_time_period['Pct Change'] < close_df_user_time_period['Pct Change'].describe()['25%'], 'Very Low', 0)\n",
    "    all_classes = []\n",
    "    for i in range(len(close_df_user_time_period)):\n",
    "        if 'Very High' in close_df_user_time_period['Pct Change Very High Class'][i]:\n",
    "            all_classes.append('Very High')\n",
    "        elif 'High' in close_df_user_time_period['Pct Change High Class'][i]:\n",
    "            all_classes.append('High')\n",
    "        elif 'Low' in close_df_user_time_period['Pct Change Low Class'][i]:\n",
    "            all_classes.append('Low')\n",
    "        elif 'Very Low' in close_df_user_time_period['Pct Change Very Low Class'][i]:\n",
    "            all_classes.append('Very Low')\n",
    "\n",
    "    close_df_user_time_period['All Classes'] = [class_ for class_ in all_classes]\n",
    "    close_df_user_time_period['Positive'] = np.where(close_df_user_time_period['Pct Change'] > 0, 'Positive', 'Negative')\n",
    "\n",
    "    sentiment_df = sentiment_df.loc[sentiment_df.index.isin(close_df_user_time_period.index)]\n",
    "    sentiment_df = sentiment_df.dropna()\n",
    "    close_df_user_time_period = close_df_user_time_period.loc[close_df_user_time_period.index.isin(sentiment_df.index)]\n",
    "    X = sentiment_df['Sentiment'].to_numpy().reshape(-1, 1)\n",
    "    y1 = close_df_user_time_period['All Classes']\n",
    "    y2 = close_df_user_time_period['Positive']\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train, X_test, y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression()\n",
    "    lr_params = [{'solver':['newton-cg', 'lbfgs', 'liblinear']}]\n",
    "    lr_clf = GridSearchCV(lr, lr_params, cv=5, scoring='accuracy')\n",
    "    lr_clf_copy = GridSearchCV(lr, lr_params, cv=5, scoring='accuracy')\n",
    "\n",
    "    lr1_clf = lr_clf.fit(X_train, y1_train)\n",
    "    lr2_clf = lr_clf_copy.fit(X_train, y2_train)\n",
    "\n",
    "    lr1_clf_pred = lr1_clf.predict(X_test)\n",
    "    lr1_clf_acc = accuracy_score(y1_test, lr1_clf_pred)\n",
    "\n",
    "    lr2_clf_pred = lr2_clf.predict(X_test)\n",
    "    lr2_clf_acc = accuracy_score(y2_test, lr2_clf_pred)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf = RandomForestClassifier()\n",
    "    forest_param = [{'max_depth':list(range(10, 20))}]\n",
    "\n",
    "    forest_clf = GridSearchCV(rf, forest_param, cv=5, scoring='accuracy')\n",
    "    forest_clf_copy = GridSearchCV(rf, forest_param, cv=5, scoring='accuracy')\n",
    "\n",
    "    forest1_clf = forest_clf.fit(X_train, y1_train)\n",
    "    forest2_clf = forest_clf_copy.fit(X_train, y2_train)\n",
    "\n",
    "    forest1_clf_pred = forest1_clf.predict(X_test)\n",
    "    forest1_clf_acc = accuracy_score(y1_test, forest1_clf_pred)\n",
    "\n",
    "    forest2_clf_pred = forest2_clf.predict(X_test)\n",
    "    forest2_clf_acc = accuracy_score(y2_test, forest2_clf_pred)\n",
    "\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf_copy = GaussianNB()\n",
    "\n",
    "    nb1_clf = nb_clf.fit(X_train, y1_train)\n",
    "    nb2_clf = nb_clf_copy.fit(X_train, y2_train)\n",
    "\n",
    "    nb1_clf_pred = nb1_clf.predict(X_test)\n",
    "    nb1_clf_acc = accuracy_score(y1_test, nb1_clf_pred)\n",
    "\n",
    "    nb2_clf_pred = nb2_clf.predict(X_test)\n",
    "    nb2_clf_acc = accuracy_score(y2_test, nb2_clf_pred)\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb_params = [{'learning_rate':[0.0001, 0.001, 0.01, 0.1], 'n_estimators':[50, 100, 200], 'criterion':['friedman_mse', 'mse']}]\n",
    "    gb_clf = GridSearchCV(gb, gb_params, cv=5, scoring='accuracy')\n",
    "    gb_clf_copy = GridSearchCV(gb, gb_params, cv=5, scoring='accuracy')\n",
    "\n",
    "    gb1_clf = gb_clf.fit(X_train, y1_train)\n",
    "    gb2_clf = gb_clf_copy.fit(X_train, y2_train)\n",
    "\n",
    "    gb1_clf_pred = gb1_clf.predict(X_test)\n",
    "    gb1_clf_acc = accuracy_score(y1_test, gb1_clf_pred)\n",
    "\n",
    "    gb2_clf_pred = gb2_clf.predict(X_test)\n",
    "    gb2_clf_acc = accuracy_score(y2_test, gb2_clf_pred)\n",
    "\n",
    "    all_accs1 = [lr1_clf_acc, forest1_clf_acc, nb1_clf_acc, gb1_clf_acc]\n",
    "    all_accs2 = [lr2_clf_acc, forest2_clf_acc, nb2_clf_acc, gb2_clf_acc]\n",
    "\n",
    "    models1 = [lr1_clf, forest1_clf, nb1_clf, gb1_clf]\n",
    "    models2 = [lr2_clf, forest2_clf, nb2_clf, gb2_clf]\n",
    "\n",
    "    best_acc1 = max(all_accs1)\n",
    "    best_acc2 = max(all_accs2)\n",
    "\n",
    "    models_preds_dct1 = dict(zip(models1, all_accs1))\n",
    "    models_preds_dct2 = dict(zip(models2, all_accs2))\n",
    "\n",
    "\n",
    "    def get_highest(dct, val):\n",
    "        for key, value in dct.items():\n",
    "            if val == value:\n",
    "                return key\n",
    "\n",
    "    best_model1 = get_highest(models_preds_dct1, best_acc1)\n",
    "\n",
    "    print('The best severity class model is:')\n",
    "    if best_model1 == lr1_clf:\n",
    "        print('Logistic Regression')\n",
    "    elif best_model1 == forest1_clf:\n",
    "        print('Random Forest')\n",
    "    elif best_model1 == nb1_clf:\n",
    "        print('Naive Bayes')\n",
    "    elif best_model1 == gb1_clf:\n",
    "        print('Gradient Boosting')\n",
    "\n",
    "    print()\n",
    "    print('The accuracy of the severity model is:')\n",
    "    print(best_acc1)\n",
    "    print()\n",
    "    best_model2 = get_highest(models_preds_dct2, best_acc2)\n",
    "\n",
    "    print('The best direction class model is:')\n",
    "    if best_model2 == lr2_clf:\n",
    "        print('Logistic Regression')\n",
    "    elif best_model2 == forest2_clf:\n",
    "        print('Random Forest')\n",
    "    elif best_model2 == nb2_clf:\n",
    "        print('Naive Bayes')\n",
    "    elif best_model2 == gb2_clf:\n",
    "        print('Gradient Boosting')\n",
    "\n",
    "    print()\n",
    "    print('The accuracy of the direction model is:')\n",
    "    print(best_acc2)\n",
    "    \n",
    "    return best_model1, best_model2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1c1b0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy):\n",
    "    \n",
    "    # Search for tweets of user's stock on a certain day\n",
    "    search = tweepy.Cursor(api.search, q=f'${user_stock}', lang='en', since=backtest_start, until=backtest_end)\n",
    "    # Limit results to 2000\n",
    "    tweets = search.items(2000)\n",
    "    # Create lists of the dates of the tweets and the comments\n",
    "    dates = []\n",
    "    comments = []\n",
    "    for tweet in tweets:\n",
    "        dates.append(tweet.created_at)\n",
    "        comments.append(tweet.text)\n",
    "        \n",
    "    tweet_df = pd.DataFrame([dates, comments]).T\n",
    "    tweet_df.columns = ['Dates', 'Comments']\n",
    "    # Convert the Date to the same format as the yfinance dates\n",
    "    tweet_df['Dates'] = tweet_df['Dates'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    tweet_df = tweet_df.set_index('Dates')\n",
    "    tweet_df.index = pd.to_datetime(tweet_df.index)\n",
    "    # The index has to be associated with a timezone to change it to EST.  The original dates are from the UTC timezone\n",
    "    tweet_df.index = tweet_df.index.tz_localize('UTC')\n",
    "    tweet_df = tweet_df.tz_convert('US/Eastern')\n",
    "    tweet_df = tweet_df.sort_index(ascending=True)\n",
    "    \n",
    "    # Use the same parameters as the twitter api search\n",
    "    y_ticker = yfinance.Ticker(user_stock)\n",
    "    back_test_close = y_ticker.history(start=backtest_start, end=backtest_end, interval='1m')['Close']\n",
    "    \n",
    "    # Make the indices the same\n",
    "    back_test_close = back_test_close.loc[back_test_close.index.isin(tweet_df.index)]\n",
    "    back_test_close = back_test_close.dropna()\n",
    "    \n",
    "    tweet_df = tweet_df.loc[tweet_df.index.isin(back_test_close.index)]\n",
    "    tweet_df = tweet_df.dropna()\n",
    "\n",
    "    tweet_df['Close'] = back_test_close\n",
    "    \n",
    "    # Get the sentiment of the tweets\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = tweet_df['Comments'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    tweet_df['Sentiment'] = sentiment\n",
    "    \n",
    "    # Get the percent change of each row\n",
    "    tweet_df['Pct Change'] = tweet_df['Close'].pct_change()\n",
    "    tweet_df = tweet_df.dropna()\n",
    "    \n",
    "    # Group by the index; get the mean of all of them\n",
    "    sentiment_df = tweet_df.groupby(tweet_df.index).mean()\n",
    "    \n",
    "    # This takes the mean of every sentiment up to that point; exactly the way the program works\n",
    "    rolling_avg_sentiment = sentiment_df['Sentiment'].expanding().mean()\n",
    "    \n",
    "    # Here we get the severity and direction predictions for each sentiment\n",
    "    severity_pred = rolling_avg_sentiment.apply(lambda x: best_model1.predict(np.array(x).reshape(1, -1))[0])\n",
    "    direction_pred = rolling_avg_sentiment.apply(lambda x: best_model2.predict(np.array(x).reshape(1, -1))[0])\n",
    "    \n",
    "    # Here we get the change from the first price for each\n",
    "    open_change = sentiment_df['Close'].apply(lambda x: (x - sentiment_df['Close'].iloc[0])/sentiment_df['Close'].iloc[0])\n",
    "    \n",
    "    # Initializing the math variables and lists\n",
    "    all_total = 0\n",
    "    buy_low_sell_high_total = 0\n",
    "#     prob_total = 0\n",
    "\n",
    "\n",
    "    all_total_list = []\n",
    "    buy_low_sell_high_total_list = []\n",
    "#     prob_total_list = []\n",
    "\n",
    "    all_buy_count_list = []\n",
    "    buy_low_sell_high_buy_count_list = []\n",
    "#     prob_buy_count_list = []\n",
    "\n",
    "    all_sell_count_list = []\n",
    "    buy_low_sell_high_sell_count_list = []\n",
    "#     prob_sell_count_list = []\n",
    "\n",
    "    for i in range(len(open_change)):\n",
    "\n",
    "        if direction_pred[i] == 'Positive' and severity_pred[i] == 'Very High':\n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] <= close_df_user_time_period['Pct Change'].describe()['75%']: \n",
    "                # This condition triggers a buy signal. Since we are buying, we subtract the stock's current price from our running total\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] >= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "                # This condition represents the highest conviction buy signal. The stock's price is very low and the direction prediction is positive and the severity prediction is very high.\n",
    "                buy_low_sell_high_total = buy_low_sell_high_total - sentiment_df['Close'][i]\n",
    "                buy_low_sell_high_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - sentiment_df['Close'][i]\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i])\n",
    "\n",
    "        elif direction_pred[i] == 'Positive' and severity_pred[i] == 'High':\n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                # This condition triggers a sale signal.  The current stock price is added to the running total, since we are selling.\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[0] >= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "        elif direction_pred[i] == 'Positive' and severity_pred[i] == 'Low': \n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "            elif open_change[i] >= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] <= close_df_user_time_period['Pct Change'].describe()['75%']: \n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "        elif direction_pred[i] == 'Positive' and severity_pred[i] == 'Very Low': \n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "            elif open_change[i] >= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] <= close_df_user_time_period['Pct Change'].describe()['75%']: \n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "        if direction_pred[i] == 'Negative' and severity_pred[i] == 'Very High':\n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] >= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "        elif direction_pred[i] == 'Negative' and severity_pred[i] == 'High':\n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] >= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "        elif direction_pred[i] == 'Negative' and severity_pred[i] == 'Low': \n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "            elif open_change[i] >= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] <= close_df_user_time_period['Pct Change'].describe()['75%']: \n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total - sentiment_df['Close'][i]\n",
    "                all_buy_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total - (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_buy_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "\n",
    "        elif direction_pred[i] == 'Negative' and severity_pred[i] == 'Very Low': \n",
    "            if open_change[i] >= close_df_user_time_period['Pct Change'].describe()['75%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "                buy_low_sell_high_total = buy_low_sell_high_total + sentiment_df['Close'][i]\n",
    "                buy_low_sell_high_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + sentiment_df['Close'][i]\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i])\n",
    "\n",
    "            elif open_change[i] >= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] <= close_df_user_time_period['Pct Change'].describe()['75%']: \n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.66))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.66))\n",
    "\n",
    "            elif open_change[i] <= close_df_user_time_period['Pct Change'].describe()['50%'] and open_change[i] >= close_df_user_time_period['Pct Change'].describe()['25%']:\n",
    "                all_total = all_total + sentiment_df['Close'][i]\n",
    "                all_sell_count_list.append(sentiment_df['Close'][i])\n",
    "#                 prob_total = prob_total + (sentiment_df['Close'][i] * (1-0.33))\n",
    "#                 prob_sell_count_list.append(sentiment_df['Close'][i] * (1-0.33))\n",
    "        \n",
    "        # We append the running totals to a list after each iteration\n",
    "        all_total_list.append(all_total)\n",
    "        buy_low_sell_high_total_list.append(buy_low_sell_high_total)\n",
    "#         prob_total_list.append(prob_total)\n",
    "    # Getting the counts of the buys and sells for later balancing\n",
    "    all_buys = len(all_buy_count_list)\n",
    "    all_sells = len(all_sell_count_list)\n",
    "    buy_low_sell_high_buys = len(all_buy_count_list)\n",
    "    buy_low_sell_high_sells = len(all_buy_count_list)\n",
    "#     prob_buys = len(prob_buy_count_list)\n",
    "#     prob_sells = len(prob_sell_count_list)\n",
    "\n",
    "    # We balance the surplus buys by selling off the difference at the end of the day's stock price\n",
    "    if all_buys != all_sells:\n",
    "        if all_buys > all_sells:\n",
    "            all_surplus = all_buys - all_sells\n",
    "            all_buys_value = all_total_list[-1] + (sentiment_df['Close'][-1] * all_surplus)\n",
    "\n",
    "        else:\n",
    "            # The same concept.  If we shorted the stock, we buy them back to balance out the surplus at the end of the day\n",
    "            all_surplus = all_sells - all_buys\n",
    "            all_buys_value = all_total_list[-1] - (sentiment_df['Close'][-1] * all_surplus)\n",
    "    else:\n",
    "        all_buys_value = all_total_list[-1]\n",
    "    \n",
    "    \n",
    "    if buy_low_sell_high_buys != buy_low_sell_high_sells:\n",
    "        if buy_low_sell_high_buys > buy_low_sell_high_sells:\n",
    "            buy_low_sell_high_surplus = buy_low_sell_high_buys - buy_low_sell_high_sells\n",
    "            buy_low_sell_high_value = buy_low_sell_high_total_list[-1] + (sentiment_df['Close'][-1] * buy_low_sell_high_surplus)\n",
    "\n",
    "        else:\n",
    "            buy_low_sell_high_surplus = buy_low_sell_high_sells - buy_low_sell_high_buys\n",
    "            buy_low_sell_high_value = buy_low_sell_high_total_list[-1] - (sentiment_df['Close'][-1] * buy_low_sell_high_surplus)\n",
    "    else:\n",
    "        buy_low_sell_high_value = buy_low_sell_high_total_list[-1]\n",
    "    \n",
    "    print('Number of buy suggestions:', all_buys)\n",
    "    print('Number of sell suggestions:', all_sells)\n",
    "    print('Number of ignore suggestions:', len(open_change) - (all_buys + all_sells))\n",
    "    print('Value after end-of-day liquidation:')\n",
    "    # These values represent the money that we earned at the end of the day by following the program's suggestions.\n",
    "    if strategy == 'All Suggestions':\n",
    "        return all_buys_value\n",
    "    elif strategy == 'Strongest Convictions':\n",
    "        return buy_low_sell_high_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "67d6a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best severity class model is:\n",
      "Naive Bayes\n",
      "\n",
      "The accuracy of the severity model is:\n",
      "0.3125\n",
      "\n",
      "The best direction class model is:\n",
      "Naive Bayes\n",
      "\n",
      "The accuracy of the direction model is:\n",
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "user_stock = 'BA'\n",
    "\n",
    "severity_model, direction_model = get_best_models(user_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e204ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buy suggestions: 60\n",
      "Number of sell suggestions: 151\n",
      "Number of ignore suggestions: 90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156.2085723876953"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_start = '2021-12-02'\n",
    "backtest_end = '2021-12-03'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'All Suggestions'\n",
    "ba_1202 = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "ba_1202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3a780951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buy suggestions: 192\n",
      "Number of sell suggestions: 0\n",
      "Number of ignore suggestions: 6\n",
      "Value after end-of-day liquidation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "213.77743530273438"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stock = 'BA'\n",
    "\n",
    "backtest_start = '2021-12-03'\n",
    "backtest_end = '2021-12-04'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'All Suggestions'\n",
    "ba_1203 = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "ba_1203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a4e39d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best severity class model is:\n",
      "Naive Bayes\n",
      "\n",
      "The accuracy of the severity model is:\n",
      "0.2765957446808511\n",
      "\n",
      "The best direction class model is:\n",
      "Naive Bayes\n",
      "\n",
      "The accuracy of the direction model is:\n",
      "0.5319148936170213\n",
      "Number of buy suggestions: 58\n",
      "Number of sell suggestions: 0\n",
      "Number of ignore suggestions: 250\n",
      "Value after end-of-day liquidation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.408172607421875"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stock = 'MSFT'\n",
    "\n",
    "severity_model, direction_model = get_best_models(user_stock)\n",
    "\n",
    "backtest_start = '2021-12-02'\n",
    "backtest_end = '2021-12-03'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'All Suggestions'\n",
    "msft_1202 = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "msft_1202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9e996e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buy suggestions: 318\n",
      "Number of sell suggestions: 0\n",
      "Number of ignore suggestions: 8\n",
      "Value after end-of-day liquidation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "529.4197082519531"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stock = 'MSFT'\n",
    "backtest_start = '2021-12-03'\n",
    "backtest_end = '2021-12-04'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'All Suggestions'\n",
    "msft_1203 = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "msft_1203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a7c28ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buy suggestions: 58\n",
      "Number of sell suggestions: 0\n",
      "Number of ignore suggestions: 250\n",
      "Value after end-of-day liquidation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stock = 'MSFT'\n",
    "backtest_start = '2021-12-02'\n",
    "backtest_end = '2021-12-03'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'Strongest Convictions'\n",
    "msft_1202_sc = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "msft_1202_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "4f12a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best severity class model is:\n",
      "Random Forest\n",
      "\n",
      "The accuracy of the severity model is:\n",
      "0.2708333333333333\n",
      "\n",
      "The best direction class model is:\n",
      "Logistic Regression\n",
      "\n",
      "The accuracy of the direction model is:\n",
      "0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "user_stock = 'TSLA'\n",
    "\n",
    "severity_model, direction_model = get_best_models(user_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a7fc5461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buy suggestions: 1\n",
      "Number of sell suggestions: 0\n",
      "Number of ignore suggestions: 66\n",
      "Value after end-of-day liquidation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.06005859375"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_start = '2021-12-02'\n",
    "backtest_end = '2021-12-03'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'All Suggestions'\n",
    "tsla_1202 = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "tsla_1202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b247b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buy suggestions: 21\n",
      "Number of sell suggestions: 0\n",
      "Number of ignore suggestions: 8\n",
      "Value after end-of-day liquidation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126.87554931640625"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_start = '2021-12-03'\n",
    "backtest_end = '2021-12-04'\n",
    "bestmodel1 = severity_model\n",
    "bestmodel2 = direction_model\n",
    "strategy = 'All Suggestions'\n",
    "tsla_1203 = back_test(user_stock, backtest_start, backtest_end, bestmodel1, bestmodel2, strategy)\n",
    "tsla_1203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e8353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
